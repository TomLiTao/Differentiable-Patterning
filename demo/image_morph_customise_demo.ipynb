{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from NCA.trainer.NCA_trainer import NCA_Trainer\n",
    "from Common.utils import load_emoji_sequence\n",
    "from Common.eddie_indexer import index_to_data_nca_type\n",
    "from NCA.trainer.data_augmenter_nca import DataAugmenter\n",
    "from NCA.model.NCA_model import NCA\n",
    "from einops import rearrange\n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as np\n",
    "import optax\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More customisation details\n",
    "- This notebook follows on from `image_morph_demo.ipynb` but with more customisation options explored\n",
    "- Included are:\n",
    "    - creating custom subclasses of `DataAugmenter`\n",
    "    - creating a `optax` optimiser to pass into `NCA_Trainer`\n",
    "    - creating `BOUNDARY_MASK` to fix some spatial structure into some of the NCA channels\n",
    "    - Setting optional parameters for the `train` method\n",
    "    - Sparsity pruning during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define stuff like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS = 16           # How many channels to use in the model\n",
    "TRAINING_STEPS = 1000   # How many steps to train for\n",
    "DOWNSAMPLE = 4          # How much to downsample the image by\n",
    "NCA_STEPS = 32          # How many NCA steps between each image in the data sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NCA(N_CHANNELS=CHANNELS,\n",
    "            KERNEL_STR=[\"ID\",\"GRAD\",\"LAP\"],\n",
    "            ACTIVATION=jax.nn.relu,\n",
    "            PADDING=\"CIRCULAR\",\n",
    "            FIRE_RATE=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "- Here we load a sequence of 3 images, so that the NCA learns to take the 1st and morph it through the other two\n",
    "- In `DataAugmenter` there are 2 important methods to overwrite: `data_init` and `data_callback`\n",
    "  - `data_init` is called once at the start of training - this just sets things up \n",
    "  - `data_callback(x,y,i) -> x,y` takes as input the initial state and target state of each training iteration, modifies them and then returns them\n",
    "    - This is useful for shifting images around, adding noise, removing segments of images to force regrowth, and to propagate intermediate states forwad throughout multi-step sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_emoji_sequence([\"crab.png\",\"microbe.png\",\"rooster.png\"],impath_emojis=\"demo_data/\",downsample=DOWNSAMPLE)\n",
    "\n",
    "print(\"(Batch, Time, Channels, Width, Height): \"+str(data.shape))\n",
    "plt.imshow(rearrange(data,\"() T C W H -> W (T H) C\" )[...,:3])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "class data_augmenter_subclass(DataAugmenter):\n",
    "    #Redefine how data is pre-processed before training\n",
    "    def data_init(self,SHARDING=None):\n",
    "        data = self.return_saved_data()\n",
    "        data = self.pad(data, 10) \t\t\n",
    "        self.save_data(data)\n",
    "        return None\n",
    "    def data_callback(self,x,y,i):\n",
    "        \"\"\"\n",
    "        Called after every training iteration to perform data augmentation and processing\t\t\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : PyTree [BATCHES] f32[N-N_steps,CHANNELS,WIDTH,HEIGHT]\n",
    "            Initial conditions\n",
    "        y : PyTree [BATCHES] f32[N-N_steps,CHANNELS,WIDTH,HEIGHT]\n",
    "            Final states\n",
    "        i : int\n",
    "            Current training iteration - useful for scheduling mid-training data augmentation\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        x : PyTree [BATCHES] f32[N-N_steps,CHANNELS,WIDTH,HEIGHT]\n",
    "            Initial conditions\n",
    "        y : PyTree [BATCHES] f32[N-N_steps,CHANNELS,WIDTH,HEIGHT]\n",
    "            Final states\n",
    "\n",
    "        \"\"\"\n",
    "        am=10\n",
    "        \n",
    "        if hasattr(self,\"PREVIOUS_KEY\"):\n",
    "            x = self.unshift(x, am, self.PREVIOUS_KEY)\n",
    "            y = self.unshift(y, am, self.PREVIOUS_KEY)\n",
    "\n",
    "        x_true,_ =self.split_x_y(1)\n",
    "                \n",
    "        propagate_xn = lambda x:x.at[1:].set(x[:-1])\n",
    "        reset_x0 = lambda x,x_true:x.at[0].set(x_true[0])\n",
    "        \n",
    "        x = jax.tree_util.tree_map(propagate_xn,x) # Set initial condition at each X[n] at next iteration to be final state from X[n-1] of this iteration\n",
    "        x = jax.tree_util.tree_map(reset_x0,x,x_true) # Keep first initial x correct\n",
    "        \n",
    "                \n",
    "        for b in range(len(x)//2):\n",
    "            x[b*2] = x[b*2].at[:,:self.OBS_CHANNELS].set(x_true[b*2][:,:self.OBS_CHANNELS]) # Set every other batch of intermediate initial conditions to correct initial conditions\n",
    "        \n",
    "        if hasattr(self, \"PREVIOUS_KEY\"):\n",
    "            key = jax.random.fold_in(self.PREVIOUS_KEY,i)\n",
    "        else:\n",
    "            key=jax.random.PRNGKey(int(time.time()))\n",
    "        x = self.shift(x,am,key=key)\n",
    "        y = self.shift(y,am,key=key)\n",
    "\n",
    "        if i < 1000:\n",
    "            x = self.zero_random_circle(x,key=key)\n",
    "        x = self.noise(x,0.005,key=key)\n",
    "        self.PREVIOUS_KEY = key\n",
    "        return x,y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Boundary mask\n",
    "- Sometimes we want to encode something about spatial structure to the NCA\n",
    "    - For example, we may be modelling complex boundary conditions that aren't a square, or we want learn two different behaviours that depend on some spatially varying parameter\n",
    "- This is done via the `BOUNDARY_MASK` variable, an array of shape `[BoundaryChannels, Width, Height]` where\n",
    "    - `BoundaryChannnels < HiddenChannels`\n",
    "    - `Width` and `Height` match the data width and height\n",
    "- During training, `BoundaryChannels` number of hidden channels will be forced to stay as the `BOUNDARY_MASK` variable\n",
    "- The mask isn't saved with the model, so when running a trained NCA afterwards, do:\n",
    "    - `nca(x,lambda x:x.at[-BoundaryChannels:].set(BOUNDARY_MASK))`\n",
    "- In this example, we set 2 channels of the `BOUNDARY_MASK` to be horizontal and vertical stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary_mask = np.zeros(data.shape[2:])[:2]\n",
    "boundary_mask = np.pad(boundary_mask, ((0,0),(10,10),(10,10)))\n",
    "print(boundary_mask.shape)\n",
    "boundary_mask = boundary_mask.at[0,::2].set(1)\n",
    "boundary_mask = boundary_mask.at[1,:,::2].set(1)\n",
    "\n",
    "plt.imshow(rearrange(boundary_mask,\"C W H -> W (C H)\"),cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer initialisation\n",
    "- Some customisation of the training process happens at initialisation time of the NCA_Trainer class\n",
    "  - `DATA_AUGMENTER` can be given any subclass of `DataAugmenter`, which allows for defining custom behaviours for data augmentation during training\n",
    "  - `BOUNDARY_MASK` defined above, needs to be passed in here\n",
    "  - `GRAD_LOSS` flags whether to apply the loss function to just the cell states, or also to their spatial derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NCA_Trainer(NCA_model=model,\n",
    "                      data = data,\n",
    "                      DATA_AUGMENTER=data_augmenter_subclass,\n",
    "                      model_filename=\"test_morph_crab_microbe_rooster\",\n",
    "                      BOUNDARY_MASK=boundary_mask,\n",
    "                      GRAD_LOSS=True,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training customisation\n",
    "- There is plenty to customise here. Firstly, any optax gradient transformation works:\n",
    "    - Here we have used a learn rate scheduler, with the NAdam optimiser with gradient block normalisation\n",
    "- There are many optional parameters for `NCA_Trainer.train`:\n",
    "    - `STATE_REGULARISER`: controls how strongly the NCA penalises any intermediate states going out of the range [0,1]\n",
    "    - `WARMUP`: how many training steps to wait before saving models\n",
    "    - `LOG_EVERY`: how frequently to log model behaviour during training. Making this too often can waste lots of storage\n",
    "    - `WRITE_IMAGES`: whether to save intermediate image states or just losses\n",
    "    - `LOSS_FUNC_STR`: string describing the loss function. Choose from: `\"l2\",\"l1\",\"vgg\",\"euclidean\",\"spectral\",\"spectral_full\",\"rand_euclidean\"`\n",
    "    - `LOOP_AUTODIFF`: controls the internal `eqx.internal.scan` kind. Choose from `\"checkpointed\"` or `\"lax\"`. `\"lax\"` is generally faster, but uses much more RAM, especially for longer numbers of `NCA_STEPS`. `\"checkpointed\"` is a bit slower but uses less RAM\n",
    "    - `SPARSITY_PRUNING`: whether to gradually prune the NCA parameters during training to a desired sparsity\n",
    "    - `TARGET_SPARSITY`: Desired sparsity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = optax.exponential_decay(1e-3, transition_steps=TRAINING_STEPS, decay_rate=0.99)\n",
    "optimiser = optax.chain(optax.scale_by_param_block_norm(),\n",
    "                        optax.nadam(schedule))\n",
    "\n",
    "\n",
    "trainer.train(t=NCA_STEPS,\n",
    "              iters=TRAINING_STEPS,\n",
    "              optimiser=optimiser,\n",
    "              STATE_REGULARISER=1.0,\n",
    "              WARMUP=100,\n",
    "              LOG_EVERY=10,\n",
    "              WRITE_IMAGES=True,\n",
    "              LOSS_FUNC_STR=\"euclidean\",\n",
    "              LOOP_AUTODIFF=\"checkpointed\",\n",
    "              SPARSE_PRUNING=True,\n",
    "              TARGET_SPARSITY=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating training:\n",
    "- In the terminal, run the following command:\n",
    "\n",
    "`tensorboard --samples_per_plugin images=200 --logdir logs/test_morph_crab_microbe_rooster/train/`\n",
    "\n",
    "- Where `test_morph_crab_microbe_rooster` is the model filename we supplied when defining the `NCA_Trainer`\n",
    "\n",
    "- Then, open your browser and go to: `http://localhost:6006/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
